#!/usr/bin/env python3
"""
Corrective RAG (CRAG) - Self-Correcting Retrieval-Augmented Generation
================================================================================

This lab implements CRAG, an advanced RAG technique that evaluates retrieval
quality and takes corrective actions when the initial retrieval is insufficient.

WHAT IS CRAG?
CRAG (Corrective RAG) adds a self-correction loop to standard RAG:
1. Retrieve documents (standard RAG)
2. Evaluate retrieval quality (new step)
3. If quality is poor, take corrective action (new step)
4. Refine knowledge from documents (new step)
5. Generate answer with corrected context

THE CRAG WORKFLOW:
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Query     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Retrieve   â”‚
                    â”‚  Documents  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Evaluate   â”‚â—„â”€â”€ Retrieval Grader
                    â”‚  Relevance  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚            â”‚            â”‚
              â–¼            â–¼            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚CORRECT â”‚  â”‚AMBIGUOUS â”‚  â”‚INCORRECTâ”‚
         â”‚  Use   â”‚  â”‚  Refine  â”‚  â”‚  Web   â”‚
         â”‚  docs  â”‚  â”‚  + Web   â”‚  â”‚ Search â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
              â”‚           â”‚             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Refine    â”‚â—„â”€â”€ Knowledge Extraction
                   â”‚  Knowledge  â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Generate   â”‚
                   â”‚   Answer    â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WHY CRAG MATTERS FOR ENTERPRISE:
- Reduces "I don't know" responses by finding alternative sources
- Self-healing: Automatically corrects retrieval failures
- Higher accuracy through relevance filtering
- Graceful degradation when knowledge base is incomplete
- Audit trail: Know when and why corrections occurred

PREREQUISITES:
   - ChromaDB populated with OmniTech documents (run index_pdfs.py first)
   - Ollama running with llama3.2:3b model (ollama pull llama3.2:3b)

USAGE:
   python lab9.py
"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# IMPORTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import requests
from typing import List, Dict, Tuple
from dataclasses import dataclass
from enum import Enum
from chromadb import PersistentClient
from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

CHROMA_PATH = "./chroma_db"
COLLECTION_NAME = "pdf_documents"
OLLAMA_URL = "http://localhost:11434/api/generate"
OLLAMA_MODEL = "llama3.2:3b"

# CRAG thresholds - tune these based on your quality requirements
RELEVANCE_THRESHOLD_HIGH = 0.7   # Above this = CORRECT (use retrieved docs)
RELEVANCE_THRESHOLD_LOW = 0.4   # Below this = INCORRECT (need web search)
                                 # Between = AMBIGUOUS (refine + supplement)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA STRUCTURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RetrievalDecision(Enum):
    """
    CRAG decision categories based on retrieval quality evaluation.

    CORRECT: Retrieved documents are highly relevant - use them directly
    INCORRECT: Retrieved documents are not relevant - need external search
    AMBIGUOUS: Partial relevance - refine documents and supplement with search
    """
    CORRECT = "correct"
    INCORRECT = "incorrect"
    AMBIGUOUS = "ambiguous"


@dataclass
class CRAGResult:
    """
    Container for CRAG pipeline results with full audit trail.

    This provides transparency into what corrective actions were taken,
    essential for enterprise debugging and compliance.
    """
    question: str
    answer: str

    # Retrieval info
    initial_chunks: List[Dict]
    relevance_scores: List[float]
    decision: RetrievalDecision

    # Corrective actions taken
    web_search_used: bool = False
    web_results: List[Dict] = None
    refined_knowledge: str = ""

    # Final context used
    final_context: List[Dict] = None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CORRECTIVE RAG CLASS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CorrectiveRAG:
    """
    Corrective RAG (CRAG) implementation with self-correction capabilities.

    This class implements the CRAG workflow:
    1. Retrieve documents from vector store
    2. Evaluate relevance of each document
    3. Make decision: CORRECT / INCORRECT / AMBIGUOUS
    4. Take corrective action if needed (web search, refinement)
    5. Extract and refine knowledge from sources
    6. Generate final answer

    Key innovation: The system "knows when it doesn't know" and takes
    corrective action rather than hallucinating or giving up.
    """

    def __init__(self):
        """Initialize the CRAG system."""
        print("Initializing Corrective RAG System...")

        # Connect to ChromaDB
        client = PersistentClient(
            path=CHROMA_PATH,
            settings=Settings(),
            tenant=DEFAULT_TENANT,
            database=DEFAULT_DATABASE
        )
        self.collection = client.get_collection(name=COLLECTION_NAME)
        print(f"  Connected to ChromaDB with {self.collection.count()} chunks")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 1: RETRIEVAL
    # Standard vector similarity search
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def retrieve(self, query: str, k: int = 5) -> List[Dict]:
        """
        Retrieve candidate documents from vector store.

        We retrieve MORE documents than typical RAG (5 vs 3) because
        CRAG will filter them based on relevance evaluation.
        """
        results = self.collection.query(
            query_texts=[query],
            n_results=k,
            include=["documents", "metadatas", "distances"]
        )

        chunks = []
        if results['documents'] and len(results['documents'][0]) > 0:
            for i in range(len(results['documents'][0])):
                chunks.append({
                    "content": results['documents'][0][i],
                    "metadata": results['metadatas'][0][i],
                    "score": 1.0 / (1.0 + results['distances'][0][i])
                })
        return chunks

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 2: RETRIEVAL EVALUATION (The "C" in CRAG)
    # Grade each document's relevance to the query
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def evaluate_relevance(self, query: str, document: str) -> float:
        """
        Evaluate how relevant a single document is to the query.

        This is the RETRIEVAL GRADER - the key component of CRAG.
        It determines whether each retrieved document actually helps
        answer the question.

        Returns:
            Float between 0.0 (irrelevant) and 1.0 (highly relevant)
        """
        eval_prompt = f"""You are a strict relevance grader. Evaluate if this document contains information that can actually ANSWER the question - not just whether it's from the same company or domain.

QUESTION: {query}

DOCUMENT: {document[:800]}

Be strict: The document must contain specific information needed to answer the question.
- Being from the same company is NOT enough
- Being about a related topic is NOT enough
- The document must have facts/data that directly help answer THIS specific question

Rate from 1-5:
1 = No relevant information (wrong topic, or right company but wrong content)
2 = Tangentially related (mentions related concepts but can't answer the question)
3 = Partially relevant (some useful info but incomplete)
4 = Relevant (contains information that helps answer the question)
5 = Highly relevant (directly answers the question with specific details)

Respond with ONLY a number (1-5):"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": eval_prompt,
                    "stream": False,
                    "options": {"temperature": 0.1}
                },
                timeout=30
            )
            if response.status_code == 200:
                result = response.json().get("response", "").strip()
                for char in result:
                    if char.isdigit():
                        return int(char) / 5.0  # Normalize to 0-1
                return 0.5
            return 0.0
        except Exception:
            return 0.0

    def evaluate_all_documents(self, query: str, chunks: List[Dict]) -> List[float]:
        """
        Evaluate relevance of all retrieved documents.

        Returns list of relevance scores, one per document.
        """
        scores = []
        for chunk in chunks:
            score = self.evaluate_relevance(query, chunk['content'])
            scores.append(score)
        return scores

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 3: DECISION MAKING
    # Decide what corrective action to take based on relevance scores
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def make_decision(self, relevance_scores: List[float]) -> RetrievalDecision:
        """
        Decide corrective action based on document relevance scores.

        Decision logic:
        - CORRECT: At least one document is highly relevant (>0.7)
        - INCORRECT: All documents are irrelevant (<0.4)
        - AMBIGUOUS: Some relevance but not strong (between thresholds)

        This is where CRAG "knows when it doesn't know."
        """
        if not relevance_scores:
            return RetrievalDecision.INCORRECT

        max_score = max(relevance_scores)
        avg_score = sum(relevance_scores) / len(relevance_scores)

        # Decision logic
        if max_score >= RELEVANCE_THRESHOLD_HIGH:
            # At least one highly relevant document - use retrieved docs
            return RetrievalDecision.CORRECT
        elif max_score < RELEVANCE_THRESHOLD_LOW:
            # No relevant documents - need external search
            return RetrievalDecision.INCORRECT
        else:
            # Partial relevance - refine and supplement
            return RetrievalDecision.AMBIGUOUS

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 4: CORRECTIVE ACTIONS
    # Take action based on the decision
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def simulate_web_search(self, query: str) -> List[Dict]:
        """
        Simulate web search for external knowledge.

        In a production system, this would:
        - Call a search API (Google, Bing, DuckDuckGo)
        - Scrape and process results
        - Return relevant snippets

        For this lab, we simulate with LLM-generated "web results"
        to demonstrate the CRAG pattern without external dependencies.
        """
        web_prompt = f"""You are simulating web search results. Generate 2-3 short, factual snippets that might be found on the web about this topic.

SEARCH QUERY: {query}

Generate realistic search result snippets (2-3 sentences each). Format as:
[Result 1]
<snippet>

[Result 2]
<snippet>

Keep responses factual and relevant to the query:"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": web_prompt,
                    "stream": False,
                    "options": {"temperature": 0.5}
                },
                timeout=60
            )
            if response.status_code == 200:
                result = response.json().get("response", "").strip()
                # Parse into chunks
                web_chunks = []
                for i, part in enumerate(result.split('[Result')):
                    if part.strip():
                        content = part.split(']')[-1].strip() if ']' in part else part.strip()
                        if content:
                            web_chunks.append({
                                "content": content,
                                "metadata": {"source": f"web_search_{i}", "type": "web"},
                                "score": 0.8  # Web results get moderate confidence
                            })
                return web_chunks[:3]  # Return at most 3 results
            return []
        except Exception as e:
            print(f"  Web search error: {e}")
            return []

    def filter_relevant_documents(self, chunks: List[Dict], scores: List[float],
                                   threshold: float = 0.4) -> List[Dict]:
        """
        Filter documents to keep only those above relevance threshold.

        This is the KNOWLEDGE REFINEMENT step for CORRECT/AMBIGUOUS cases.
        We don't just use all retrieved docs - we filter to only relevant ones.
        """
        filtered = []
        for chunk, score in zip(chunks, scores):
            if score >= threshold:
                chunk_copy = chunk.copy()
                chunk_copy['relevance_score'] = score
                filtered.append(chunk_copy)
        return filtered

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 5: KNOWLEDGE REFINEMENT
    # Extract and refine key information from sources
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def refine_knowledge(self, query: str, chunks: List[Dict]) -> str:
        """
        Refine and extract key knowledge from documents.

        This step:
        - Extracts only the relevant portions of each document
        - Removes redundancy across documents
        - Creates a focused knowledge summary

        This is the KNOWLEDGE REFINEMENT component of CRAG.
        """
        if not chunks:
            return ""

        # Combine document content
        combined = "\n\n---\n\n".join([
            f"[{c['metadata'].get('source', 'doc')}]\n{c['content']}"
            for c in chunks
        ])

        refine_prompt = f"""Extract and summarize ONLY the information relevant to answering this question.

QUESTION: {query}

DOCUMENTS:
{combined[:3000]}

Extract key facts and information that directly help answer the question.
Remove any irrelevant information. Be concise but complete.

RELEVANT INFORMATION:"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": refine_prompt,
                    "stream": False,
                    "options": {"temperature": 0.2}
                },
                timeout=60
            )
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return combined[:1500]  # Fallback to truncated original
        except Exception:
            return combined[:1500]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # STEP 6: ANSWER GENERATION
    # Generate final answer with refined context
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def generate_answer(self, question: str, refined_knowledge: str,
                        decision: RetrievalDecision) -> str:
        """
        Generate the final answer using refined knowledge.

        The prompt adapts based on the decision:
        - CORRECT: High confidence, use retrieved knowledge
        - INCORRECT: Used web search, acknowledge external source
        - AMBIGUOUS: Mixed sources, balanced response
        """
        # Adapt prompt based on decision
        if decision == RetrievalDecision.CORRECT:
            confidence_note = "The following information is from our knowledge base and is reliable."
        elif decision == RetrievalDecision.INCORRECT:
            confidence_note = "The information below is from external sources as our knowledge base didn't contain relevant information."
        else:
            confidence_note = "The information combines our knowledge base with supplementary sources."

        prompt = f"""Answer the question based on the provided information.

{confidence_note}

INFORMATION:
{refined_knowledge}

QUESTION: {question}

Provide a helpful, accurate answer. If the information is incomplete, acknowledge what you don't know.

ANSWER:"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3}
                },
                timeout=120
            )
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return f"Error generating answer"
        except Exception as e:
            return f"Error: {e}"

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MAIN CRAG PIPELINE
    # Orchestrate all steps
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def query(self, question: str) -> CRAGResult:
        """
        Execute the full CRAG pipeline.

        This orchestrates all steps:
        1. Retrieve documents
        2. Evaluate relevance of each
        3. Make decision (CORRECT/INCORRECT/AMBIGUOUS)
        4. Take corrective action if needed
        5. Refine knowledge
        6. Generate answer

        Returns CRAGResult with full audit trail.
        """
        print(f"\n{'='*60}")
        print(f"CRAG Query: {question}")
        print('='*60)

        # Step 1: Retrieve
        print("\n[1/6] Retrieving documents...")
        chunks = self.retrieve(question, k=5)
        print(f"      Retrieved {len(chunks)} documents")

        # Step 2: Evaluate relevance
        print("\n[2/6] Evaluating document relevance...")
        relevance_scores = self.evaluate_all_documents(question, chunks)
        for i, (chunk, score) in enumerate(zip(chunks, relevance_scores)):
            source = chunk['metadata'].get('source', 'unknown').split('/')[-1]
            print(f"      Doc {i+1}: {source} - Relevance: {score:.2f}")

        # Step 3: Make decision
        print("\n[3/6] Making retrieval decision...")
        decision = self.make_decision(relevance_scores)
        print(f"      Decision: {decision.value.upper()}")

        # Step 4: Take corrective action
        web_results = []
        web_search_used = False

        if decision == RetrievalDecision.CORRECT:
            print("\n[4/6] Using retrieved documents (high relevance)")
            # Filter to only relevant documents
            filtered_chunks = self.filter_relevant_documents(chunks, relevance_scores, 0.5)
            final_chunks = filtered_chunks

        elif decision == RetrievalDecision.INCORRECT:
            print("\n[4/6] Retrieval insufficient - performing web search...")
            web_results = self.simulate_web_search(question)
            web_search_used = True
            print(f"      Retrieved {len(web_results)} web results")
            final_chunks = web_results  # Use only web results

        else:  # AMBIGUOUS
            print("\n[4/6] Partial relevance - refining + supplementing...")
            # Keep somewhat relevant docs
            filtered_chunks = self.filter_relevant_documents(chunks, relevance_scores, 0.3)
            # Also do web search
            web_results = self.simulate_web_search(question)
            web_search_used = True
            print(f"      Kept {len(filtered_chunks)} docs, added {len(web_results)} web results")
            final_chunks = filtered_chunks + web_results

        # Step 5: Refine knowledge
        print("\n[5/6] Refining knowledge...")
        refined_knowledge = self.refine_knowledge(question, final_chunks)
        print(f"      Extracted {len(refined_knowledge)} chars of refined knowledge")

        # Step 6: Generate answer
        print("\n[6/6] Generating answer...")
        answer = self.generate_answer(question, refined_knowledge, decision)

        # Build result
        result = CRAGResult(
            question=question,
            answer=answer,
            initial_chunks=chunks,
            relevance_scores=relevance_scores,
            decision=decision,
            web_search_used=web_search_used,
            web_results=web_results if web_results else None,
            refined_knowledge=refined_knowledge,
            final_context=final_chunks
        )

        return result

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # COMPARISON WITH STANDARD RAG
    # Show the difference CRAG makes
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def standard_rag_query(self, question: str) -> str:
        """
        Standard RAG for comparison (no correction).

        This shows what happens WITHOUT CRAG - the system just uses
        whatever it retrieves, even if irrelevant.
        """
        chunks = self.retrieve(question, k=3)

        if not chunks:
            return "I couldn't find relevant information."

        context = "\n\n".join([c['content'] for c in chunks])

        prompt = f"""Answer based on the context.

CONTEXT:
{context[:2000]}

QUESTION: {question}

ANSWER:"""

        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": 0.3}
                },
                timeout=120
            )
            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return "Error"
        except Exception as e:
            return f"Error: {e}"

    def compare_with_standard(self, question: str) -> Dict:
        """
        Compare CRAG vs standard RAG on the same question.
        """
        print(f"\n{'='*60}")
        print("Comparing CRAG vs Standard RAG")
        print('='*60)

        # Standard RAG
        print("\n[Standard RAG]")
        standard_answer = self.standard_rag_query(question)

        # CRAG
        print("\n[Corrective RAG]")
        crag_result = self.query(question)

        return {
            'question': question,
            'standard_answer': standard_answer,
            'crag_result': crag_result
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TERMINAL COLORS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BLUE = "\033[94m"
CYAN = "\033[96m"
MAGENTA = "\033[95m"
RESET = "\033[0m"
BOLD = "\033[1m"

DECISION_COLORS = {
    RetrievalDecision.CORRECT: GREEN,
    RetrievalDecision.INCORRECT: RED,
    RetrievalDecision.AMBIGUOUS: YELLOW
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERACTIVE DEMO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_demo():
    """
    Run interactive CRAG demo.
    """
    print(f"\n{BOLD}Corrective RAG (CRAG) - Self-Correcting Retrieval{RESET}")
    print("="*55)
    print("RAG that knows when it doesn't know - and fixes it")
    print("="*55)

    crag = CorrectiveRAG()

    print(f"\n{BOLD}CRAG Decisions:{RESET}")
    print(f"  {GREEN}CORRECT{RESET}   - High relevance, use retrieved docs")
    print(f"  {YELLOW}AMBIGUOUS{RESET} - Partial relevance, refine + supplement")
    print(f"  {RED}INCORRECT{RESET} - Low relevance, use web search")

    while True:
        print(f"\n{BOLD}Options:{RESET}")
        print("  1. Run CRAG query")
        print("  2. Compare CRAG vs Standard RAG")
        print("  3. Exit")

        choice = input(f"\n{BOLD}Choose (1-3):{RESET} ").strip()

        if choice == "1":
            question = input(f"\n{BOLD}Enter question:{RESET} ").strip()
            if not question:
                continue

            result = crag.query(question)

            # Display results
            print(f"\n{'â”€'*60}")
            print(f"{BOLD}CRAG RESULT{RESET}")
            print('â”€'*60)

            # Decision
            decision_color = DECISION_COLORS.get(result.decision, RESET)
            print(f"\n{BOLD}Decision:{RESET} {decision_color}{result.decision.value.upper()}{RESET}")

            # Relevance scores
            print(f"\n{BOLD}Document Relevance:{RESET}")
            for i, (chunk, score) in enumerate(zip(result.initial_chunks, result.relevance_scores)):
                source = chunk['metadata'].get('source', 'unknown').split('/')[-1]
                bar = "â–ˆ" * int(score * 10) + "â–‘" * (10 - int(score * 10))
                color = GREEN if score >= 0.7 else YELLOW if score >= 0.4 else RED
                print(f"  {i+1}. [{bar}] {color}{score:.2f}{RESET} - {source}")

            # Web search
            if result.web_search_used:
                print(f"\n{CYAN}ğŸŒ Web search was used to supplement knowledge{RESET}")

            # Answer
            print(f"\n{BOLD}Answer:{RESET}")
            print(f"  {result.answer}")

        elif choice == "2":
            question = input(f"\n{BOLD}Enter question:{RESET} ").strip()
            if not question:
                continue

            comparison = crag.compare_with_standard(question)

            # Display comparison
            print(f"\n{'='*60}")
            print(f"{BOLD}COMPARISON RESULTS{RESET}")
            print('='*60)

            print(f"\n{RED}{BOLD}[STANDARD RAG]{RESET}")
            print(f"  {comparison['standard_answer']}")

            result = comparison['crag_result']
            decision_color = DECISION_COLORS.get(result.decision, RESET)

            print(f"\n{GREEN}{BOLD}[CORRECTIVE RAG]{RESET} (Decision: {decision_color}{result.decision.value}{RESET})")
            if result.web_search_used:
                print(f"  {CYAN}(Used web search for supplemental info){RESET}")
            print(f"  {result.answer}")

        elif choice == "3":
            print("\nGoodbye!")
            break

        else:
            print("Invalid choice. Please enter 1, 2, or 3.")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN ENTRY POINT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    run_demo()
