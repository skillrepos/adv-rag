# Enterprise AI Accelerator
## Day 1 - Models and Retrieval Augmented Generation (RAG)
## Session labs 
## Revision 1.2 - 11/28/25

**Follow the startup instructions in the README.md file IF NOT ALREADY DONE!**

**NOTE: To copy and paste in the codespace, you may need to use keyboard commands - CTRL-C and CTRL-V. Chrome may work best for this.**

**Lab 1 - Working with Neural Networks**

**Purpose: In this lab, we’ll learn more about neural networks by seeing how one is coded and trained.**

1. In our repository, we have a set of Python programs to help us illustrate and work with concepts in the labs. The first set are in the *llm* subdirectory. Go to the *TERMINAL* tab in the bottom part of your codespace and change into that directory.

```
cd llm
```
<br><br>

2. For this lab, we have a simple neural net coded in Python. The file name is nn.py. Open the file either by clicking on [**llm/nn.py**](./llm/nn.py) or by entering the command below in the codespace's terminal.

```
code nn.py
```
<br><br>


3. Scroll down to around line 55. Notice the *training_inputs* data and the *training_outputs* data. Each row of the *training_outputs* is what we want the model to predict for the corresponding input row. As coded, the output for the sample inputs ends up being the same as the first element of the array.  For inputs [0,0,1] we are trying to train the model to predict [0]. For the inputs [1,0,1], we are trying to train the model to predict [1], etc. The table below may help to explain.

| **Dataset** | **Values** | **Desired Prediction** |
| :---------: | :--------: | :--------------------: |
| **1** |  0  0  1  |            0           |
| **2** |  1  1  1  |            1           |
| **3** |  1  0  1  |            1           |
| **4** |  0  1  1  |            0           |

![Code in simple nn](./images/aia-1-3.png?raw=true "Code in simple nn")

<br><br>

4. When we run the program, it will train the neural net to try and predict the outputs corresponding to the inputs. You will see the random training weights to start and then the adjusted weights to make the model predict the output. You will then be prompted to put in your own training data. We'll look at that in the next step. For now, go ahead and run the program (command below) but don't put in any inputs yet. Just notice how the weights have been adjusted after the training process.

```
python nn.py
```
![Starting run of simple nn](./images/aia-1-4.png?raw=true "Starting run of simple nn") 

<br><br>

5. What you should see is that the weights after training are now set in a way that makes it more likely that the result will match the expected output value. (The higher positive value for the first weight means that the model has looked at the training data and realized it should "weigh" the first input higher in its prediction.) To prove this out, you can enter your own input set - just use 1's and 0's for each input. 

![Inputs to simple nn](./images/aia-1-5.png?raw=true "Inputs to simple nn") 

<br><br>

6. After you put in your inputs, the neural net will process your input and because of the training, it should predict a result that is close to the first input value you entered (the one for *Input one*).

![Prediction close to first input](./images/aia-1-6.png?raw=true "Prediction close to first input") 

<br><br>

7. Now, let's see what happens if we change the expected outputs to be different. In the editor for the *nn.py* file, find the line for the *training_outputs*. Modify the values in the array to be ([[0],[1],[0],[1]]). These are the values of the second element in each of the training data entries. After you're done, **save your changes**. (You can use the CMD/CTRL+S keyboard shortcut.)

![Modifying expected outputs](./images/aia-1-7.png?raw=true "Modifying expected outputs")

<br><br>

8. Now, run the neural net again. This time when the weights after training are shown, you should see a bias for a higher weight for the second item.
```
python nn.py
```
![Second run of simple nn](./images/aia-1-8.png?raw=true "Second run of simple nn") 

<br><br>

9. At the input prompts, just input any sequence of 0's and 1's as before.

<br><br>

10. When the trained model then processes your inputs, you should see that it predicts a value that is close to 0 or 1 depending on what your second input was.

![Second output of simple nn](./images/aia-1-9.png?raw=true "Second output of simple nn")

<br><br>

11. (Optional) If you get done early and want more to do, feel free to try other combinations of training inputs and training outputs.
    
<p align="center">
**[END OF LAB]**
</p>
</br></br>

**Lab 2 - Experimenting with Tokenization**

**Purpose: In this lab, we'll see how different models do tokenization.**

1. In the same *llm* directory, we have a simple program that can load a model and print out tokens generated by it. The file name is *tokenizer.py*. You can view the file either by clicking on [**llm/tokenizer.py**](./llm/tokenizer.py) or by entering the command below in the codespace's terminal (assuming you're still in the *genai* directory).

```
code tokenizer.py
```

<br><br>

2. This program can be run and passed a model to use for tokenization. To start, we'll be using a model named *bert-base-uncased*. Let's look at this model on huggingface.co.  Go to https://huggingface.co/models and in the *Models* search area, type in *bert-base-uncased*. Select the entry for *google-bert/bert-base-uncased*.

![Finding bert model on huggingface](./images/aia-1-10.png?raw=true "Finding bert model on huggingface")

<br><br>

3. Once you click on the selection, you'll be on the *model card* tab for the model. Take a look at the model card for the model and then click on the *Files and Versions* and *Community* tabs to look at those pages.

![huggingface tabs](./images/aia-1-11.png?raw=true "huggingface tabs")

<br><br>

4. Now let's switch back to the codespace and, in the terminal, run the *tokenizer* program with the *bert-base-uncased* model. Enter the command below. This will download some of the files you saw on the *Files* tab for the model in HuggingFace.

```
python tokenizer.py bert-base-uncased
```

<br><br>

5. After the program starts, you will be at a prompt to *Enter text*. Enter in some text like the following to see how it will be tokenized.

```
This is sample text for tokenization and text for embeddings.
```

![input for tokenization](./images/aia-1-12.png?raw=true "input for tokenization")

<br><br>

6. After you enter this, you'll see the various subword tokens that were extracted from the text you entered. And you'll also see the ids for the tokens stored in the model that matched the subwords.

![tokenization output](./images/aia-1-13.png?raw=true "tokenization output")

<br><br>

7. Next, you can try out some other models by repeating steps 4 - 6 for other tokenizers like the following. (You can use the same text string or different ones. Notice how the text is broken down depending on the model and also the meta-characters.)
```
python tokenizer.py roberta-base
python tokenizer.py gpt2
python tokenizer.py xlnet-large-cased
```

<br><br>

8. (Optional) If you finish early and want more to do, you can look up the models from step 7 on huggingface.co/models.

<br>  
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 3 - Understanding embeddings, vectors and similarity measures**

**Purpose: In this lab, we'll see how tokens get mapped to vectors and how vectors can be compared.**

1. In the repository, we have a Python program that uses a Tokenizer and Model to create embeddings for three terms that you input. It then computes and displays the cosine similarity between each combination. Open the file to look at it by clicking on [**llm/vectors.py**](./llm/vectors.py) or by using the command below in the terminal.


```
code vectors.py
```
<br><br>

2. Let's run the program. As we did for the tokenizer example, we'll pass in a model to use. We'll also pass in a second argument which is the number of dimensions from the vector for each term to show. Run the program with the command below. You can wait to enter terms until the next step.


```
python vectors.py bert-base-cased 5
```

![vectors program run](./images/aia-1-14.png?raw=true "vectors program run")

<br><br>

3. The command we just ran loads up the bert-base-cased model and tells it to show the first 5 dimensions of each vector for the terms we enter. The program will be prompting you for three terms. Enter each one in turn. You can try two closely related words and one that is not closely related. For example
   - king
   - queen
   - duck

![vectors program inputs](./images/aia-1-15.png?raw=true "vectors program inputs")

<br><br>

4. Once you enter the terms, you'll see the first 5 dimensions for each term. And then you'll see the cosine similarity displayed between each possible pair. This is how similar each pair of words is. The two that are most similar should have a higher cosine similarity "score".

![vectors program outputs](./images/aia-1-16.png?raw=true "vectors program outputs")

<br><br>

5. Each vector in the bert-based models have 768 dimensions. Let's run the program again and tell it to display 768 dimensions for each of the three terms.  Also, you can try another set of terms that are more closely related, like *multiplication*, *division*, *addition*.

```
python vectors.py bert-base-cased 768
```

<br><br>

6. You should see that the cosine similarities for all pair combinations are not as far apart this time.

![vectors program second outputs](./images/aia-1-17.png?raw=true "vectors program second outputs")

<br><br>

7. As part of the output from the program, you'll also see the *token id* for each term. (It is above the print of the dimensions. If you don't want to scroll through all the dimensions, you can just run it again with a small number of dimensions like we did in step 2.) If you're using the same model as you did in lab 2 for tokenization, the ids will be the same. 

![token id](./images/aia-1-18.png?raw=true "token id")

<br><br>


8. You can actually see where these mappings are stored if you look at the model on Hugging Face. For instance, for the *bert-base-cased* model, you can go to https://huggingface.co and search for bert-base-cased. Select the entry for google-bert/bert-base-cased. (Make sure you pick the one with that name.)

![finding model](./images/aia-1-19.png?raw=true "finding model")

<br><br>

9. On the page for the model, click on the *Files and versions* tab. Then find the file *tokenizer.json* and click on it. The file will be too large to display, so click on the *check the raw version* link to see the actual content.

![selecting tokenizer.json](./images/aia-1-20.png?raw=true "selecting tokenizer.json")
![opening file](./images/aia-1-21.png?raw=true "opening file")

<br><br>

9. You can search for the terms you entered previously with a Ctrl-F or Cmd-F and find the mapping between the term and the id. If you look for "##" you'll see mappings for parts of tokens like you may have seen in lab 2.

![finding terms in file](./images/aia-1-22.png?raw=true "finding terms in files")

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 4 - Working with transformer models**

**Purpose: In this lab, we’ll see how to interact with various models for different standard tasks**

1. In our repository, we have several different Python programs that utilize transformer models for standard types of LLM tasks. These programs have some random facts from different categories stored in them to use for transformer models to act on.

<br><br>

2. One of the programs is a simple translation example. The file name is *translation.py*. Open the file either by clicking on [**llm/translation.py**](./llm/translation.py) or by entering the command below in the codespace's terminal. 

```
code translation.py
```
<br><br>

3. Take a look at the file contents.  Notice that we are pulling in a specific model ending with 'en-fr'. This is a clue that this model is trained for English to French translation. Let's find out more about it. In a browser, go to *https://huggingface.co/models* and search for the model name 'Helsinki-NLP/opus-mt-en-fr' (or you can just go to huggingface.co/Helsinki-NLP/opus-mt-en-fr).

![model search](./images/aia-1-23.png?raw=true "model search")

<br><br>

3. You can look around on the model card for more info about the model. Notice that it has links to an *OPUS readme* and also links to download its original weights, translation test sets, etc.

<br><br>

4. When done looking around, go back to the repository and look at the rest of the *translation.py* file. What we are doing is loading the model, the tokenizer, and then taking a set of random texts and running them through the tokenizer and model to do the translation. Go ahead and execute the code in the terminal via the command below.

```
python translation.py
```

![translation by model](./images/aia-1-24.png?raw=true "translation by model")
 
<br><br>

5. There's also an example program for doing classification. The file name is classification.py. Open the file either by clicking on [**llm/classification.py**](./llm/classification.py) or by entering the command below in the codespace's terminal.


```
code classification.py
```

<br><br>

6. Take a look at the model for this one *joeddav/xlm-roberta-large-xnli* on huggingface.co and read about it. When done, come back to the repo.

<br><br>

7. *classification.py* uses a HuggingFace pipeline to do the main work. Notice it also includes a list of categories as *candidate_labels* that it will use to try and classify the data. Go ahead and run it to see it in action. (This will take awhile to download the model.) After it runs, you will see each topic, followed by the ratings for each category. The scores reflect how well the model thinks the topic fits a category. The highest score reflects which category the model thinks fit best.

```
python classification.py
```

![classification by model](./images/aia-1-25.png?raw=true "classification by model")

<br><br>

8. Finally, we have a program to do sentiment analysis. The file name is sentiment.py. Open the file either by clicking on [**llm/sentiment.py**](./llm/sentiment.py) or by entering the command below in the codespace's terminal.

```
code sentiment.py
```

<br><br>

9. Again, you can look at the model used by this one *distilbert-base-uncased-finetuned-sst-2-english* in Hugging Face.

<br><br>

10. When ready, go ahead and run this one in the similar way and observe which ones it classified as positive and which as negative and the relative scores.

```
python sentiment.py
```

![sentiment by model](./images/aia-1-26.png?raw=true "sentiment by model")

<br><br>

11. If you're done early, feel free to change the texts, the candidate_labels in the previous model, etc. and rerun the models to see the results.

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 5 - Fine-tuning Models**

**Purpose: In this lab, we'll see how we can fine-tune a model with a set of data to get better responses in a particular domain.**

1. For this lab, we will build out a starter file into a full fine-tuning example. The starter file is in the ft (for "fine tuning") directory. Change into that directory.

```
cd /workspaces/aia-day1/ft
```

<br><br>

2. We'll be tuning a basic model from Hugging Face called *distilbert-base-uncased*. You can see information about that model [here](https://huggingface.co/distilbert/distilbert-base-uncased).

<br><br>

3. We'll be using that model to do sentiment analysis on Amazon product reviews. Sentiment analysis means determining if a review is positive, negative, or other - as we did with one of the examples in Lab 4. The dataset we'll be using for testing and fine-tuning is a *top-level* one on Hugging Face named "Amazon Review Polarity". We can't see that version on Hugging Face, but there is one based on that that you can look at [here](https://huggingface.co/datasets/mteb/amazon_polarity). You can use the *Dataset Viewer* on that page to see the info in the dataset if interested.

![dataset viewer](./images/aia-1-27.png?raw=true "dataset viewer")

<br><br>

4. To build out the actual fine-tuning demo, we'll use a side-by-side compare and merge approach. That means we'll start up an editor with a completed version of the file on the left and the starter version on the right. To do this, run the command below: (the complete code is in [extra/reviews-ft.txt](./extra/reviews-ft.txt)).


```
code -d ../extra/reviews-ft.txt reviews-ft.py
```

![diff](./images/aia-1-28.png?raw=true "diff")

<br><br>

5. After running this command, you'll see the side-by-side editors. Now we want to merge in the sections that are in the left file into the right file to build out our demo. Before you merge in a section, make sure to glance at the block of code to try and understand what it's doing. Then, when ready, hover over the bar between the two versions and an arrow should display. Click on the arrow to merge that section in.

![review and merge](./images/aia-1-29.png?raw=true "review and merge")

<br><br>


6. Proceed down through the remaining differences, quickly reviewing the code to be merged, and then merging it with the arrow. Once you are done, the files should show as the same without any remaining "blocks" of differences. When there are no differences left and you're done, close the diff view by clicking on the "X" in the tab at the top.

![close after merging](./images/aia-1-30.png?raw=true "close after merging")

<br><br>


7.  Now we can run the demo with the following command:

```
python reviews-ft.py
```

![running](./images/aia-1-31.png?raw=true "running")

<br><br>

8. This will first download the Distilbert model and then run through a subset of reviews to see how well the model does (without any fine-tuning) on determining the sentiment of each review. If this goes by too fast, you can scroll back up to see the reviews and results. You will probably see something in the 40-50% success range.

![no fine tuning run](./images/aia-1-32.png?raw=true "no fine tuning run")

<br><br>

9. Now the model will use some Hugging Face transformer library tools to train the model from the dataset. This part will take probably as much as 5-8 minutes. What you are looking for here is the *loss value* to go down as the fine tuning proceeds. The loss value going down means that the model is getting better at predicting the sentiment as it learns from the dataset examples. 

![fine tuning run](./images/aia-1-33.png?raw=true "fine tuning run")

<br><br>

10. At the end of the fine-tuning, the program will once again run through a test of the model's prediction for sentiments on the reviews. This time, since it has been fine-tuned, it should report success more in the 80-90% range.

![after fine tuning run](./images/aia-1-34.png?raw=true "after fine tuning run")

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 6 - Working with Vector Databases**

**Purpose: In this lab, we’ll learn about how to use vector databases for storing supporting data and doing similarity searches.**

1. For this lab and the following ones, we'll be using files in the *rag* subdirectory. Change to that directory.

```
cd rag
```

<br><br>

2. We have several data files that we'll be using that are for a ficticious company. The files are located in the *rag/knowledge_base_pdfs* directory. [knowledge base pdfs](./rag/knowledge_base_pdfs) You can browse them via the explorer view. Here's a [direct link](./rag/knowledge_base_pdfs/OmniTech_Returns_Policy_2024.pdf) to an example one if you want to open it and take a look at it.

![PDF data file](./images/aia-1-41.png?raw=true "PDF data file") 

<br><br>

3. In our repository, we have some simple tools built around a popular vector database called Chroma. There are two files which will create a vector db (index) for the *.py files in our repo and another to do the same for the pdfs in our knowledge base. You can look at the files either via the usual "code <filename>" method or clicking on [**tools/index_code.py**](./tools/index_code.py) or [**tools/index_pdfs.py**](./tools/index_pdfs.py).

```
code ../tools/index_code.py
code ../tools/index_pdfs.py
```

<br><br>

4. Let's create a vector database of our local code files (python and bash). Run the program to index those. **This may run for a while before you see things happening.** You'll see the program loading the embedding model that will turn the code chunks into numeric represenations in the vector database and then it will read and index our *.py files. It will create a new local vector database in *./chroma_code_db*.

```
python ../tools/index_code.py
```

![Running code indexer](./images/aia-1-35.png?raw=true "Running code indexer")

![Running code indexer](./images/aia-1-36.png?raw=true "Running code indexer")

<br><br>

5. To help us do easy/simple searches against our vector databases, we have another tool at [**tools/search.py**](./tools/search.py). This tool connects to the ChromaDB vector database we create, and, using cosine similarity metrics, finds the top "hits" (matching chunks) and prints them out. You can open it and look at the code in the usual way if you want. No changes are needed to the code.

```
code ../tools/search.py
```

This tool takes a *--target* argument when you run it with a value of either "code" or "pdfs" to indicate which vector database to search.
You can also pass search queries directly on the command line with the *--query* argument. Or you can just start it and type in the queries, hit return, and get results. To exit in that mode, type "exit".

<br><br>

6. Now, let's run the search tool against the vector database we built in step 4. You can run it with phrases related to our coding like any of the ones shown below. You can run the commands with separate invocations of the tool as shown here, or just run it and enter them in interactive mode.  Notice the top hits and their respective cosine similarity values. Are they close? Farther apart?

```
  python ../tools/search.py --query "convert text to vectors" --target code
  python ../tools/search.py --query "tokenize sentences" --target code
  python ../tools/search.py --query "convert text to numbers" --target code
```

![Running search](./images/aia-1-37.png?raw=true "Running search")

<br><br>

7.  Now, let's create a vector database based off of the PDF files. Just run the indexer for the pdf file.

```
python ../tools/index_pdfs.py
```

![Indexing PDFs](./images/aia-1-38.png?raw=true "Indexing PDFs")

<br><br>

8. Now, we can run the same search tool to find the top hits for information about the company policies. Below are some prompts you can try here. Notice the cosine similarity values on each - are they close? Farther apart?  When done, just type "exit".

```
  python ../tools/search.py --query "track my shipment" --target pdfs
  python ../tools/search.py --query "forgot my login credentials" --target pdfs
  python ../tools/search.py --query "exchange damaged item" --target pdfs
```

![PDF search](./images/aia-1-40.png?raw=true "PDF search")

<br><br>

8. Keep in mind that this is not trying to intelligently answer your prompts at this point. This is a simple semantic search to find related chunks. In lab 7, we'll add in the LLM to give us better responses. 

<br>
<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


**Lab 7: Building a Complete RAG System**

**Purpose: In this lab, we'll create a complete RAG (Retrieval-Augmented Generation) system that retrieves relevant context from our vector database and uses an LLM to generate intelligent, grounded answers.**

1. You should still be in the *rag* subdirectory. We're going to build a TRUE RAG system that combines vector search with LLM generation. This is different from Lab 6 - instead of just finding similar chunks, we'll use those chunks as context for an LLM to generate complete answers. First, we need to bring down a smaller model to use with these labs. Use the Ollama command below:

```
ollama pull llama3.2:1b
```

<br><br>

2. Now let's examine our complete RAG implementation. We have a completed version and a skeleton version. Use the diff command to see the differences:

```
code -d ../extra/rag_complete.txt rag_code.py
```

![Diff](./images/aia-1-42.png?raw=true "Diff")

<br><br>

3. Once you have the diff view open, take a moment to look at the structure in the complete version on the left. Notice the three main methods: `retrieve()` for finding chunks, `build_prompt()` for augmenting with context, and `generate()` for calling the LLM. These are the three steps of RAG.

- Lines 95-157: `retrieve()` - semantic search in ChromaDB
- Lines 159-209: `build_prompt()` - combining context with the question
- Lines 211-273: `generate()` - calling Ollama's Llama 3.2 model

<br><br>

4. Now, as you've done before, merge the code segments from the complete file (left side) into the skeleton file (right side) by clicking the arrow pointing right in the middle bar for each difference. Start with the comments section at the top, then work your way down through the class methods.

![Merge](./images/aia-1-43.png?raw=true "Merge")

<br><br>

5. After merging all the changes, double-check that there are no remaining diffs (red blocks on the side). Then close the diff view by clicking the "X" in the tab. 

![Completed](./images/aia-1-44.png?raw=true "Completed")

<br><br>

6. Now let's run our complete RAG system:

```
python rag_code.py
```

The system will connect to the vector database we created in Lab 6 and check if Ollama is running.

<br><br>

7. You should see knowledge base statistics showing how many chunks are indexed, and a check that Ollama is running with the llama3.2:1b model. If you see any errors about Ollama not running, check that with "ollama list".  If Ollama doesn't respond, try "ollama serve &".

![Running](./images/aia-1-45.png?raw=true "Running")

<br><br>

8. Now you'll be at a prompt to ask questions. Try this first question:

```
How can I return a product?
```

Watch what happens - the system will show you the three RAG steps in the logs:
- **[RETRIEVE]** Finding relevant chunks in the vector database
- **[AUGMENT]** Building a prompt with context
- **[GENERATE]** Querying Llama 3.2 to generate an answer

<br><br>

9. After a few seconds, you'll see an ANSWER section with the LLM-generated response, followed by a SOURCES section showing which PDFs and pages were used. Notice how the answer is much more complete and natural than just showing search results.

![Answer](./images/aia-1-47.png?raw=true "Answer")

<br><br>

10. Try a few more questions to see RAG in action:

```
What are the shipping costs?
How do I reset my password?
What should I do if my device won't turn on?
```

For each question, notice how the system retrieves relevant chunks and generates a complete answer based on that context.

![Answer](./images/aia-1-46.png?raw=true "Answer")

<br><br>

11. Now try asking a question that's NOT in the PDFs to see how RAG handles it:

```
What's the CEO's favorite color?
```

![Answer](./images/aia-1-48.png?raw=true "Answer")

Notice how the system should say it doesn't have that information (rather than making something up). This is the "grounding" benefit of RAG - answers are based on actual documents.

<br><br>

12. When you're done experimenting, type `quit` to exit the system.

<br><br>


**Key Takeaways:**
- You've built a TRUE RAG system that combines vector search with LLM generation
- RAG has three steps: Retrieve relevant chunks, Augment the prompt with context, Generate answers with an LLM
- The system uses ChromaDB for semantic search and Ollama/Llama 3.2 for generation
- RAG answers are grounded in your documents - reducing hallucination compared to pure LLM queries
- The system can cite sources, showing which documents and pages were used

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>

**Lab 8: Tuning RAG - Temperature and Context**

**Purpose: In this lab, we'll learn how to control RAG quality by tuning retrieval settings and temperature parameters.**

1. You should still be in the *rag* subdirectory. Open the RAG code file:

```
code rag_code.py
```
<br><br>

2. Find line 465 where it says `result = rag.query(question, max_context_chunks=3)`. This controls how many chunks are retrieved. Try changing `3` to `1`, save, and run:

```
python rag_code.py
```

Ask: `How can I return a product?`

Notice the answer is incomplete - not enough context! Type `quit` to exit.

![Mod](./images/aia-1-49.png?raw=true "Mod")

<br><br>

3. Change `max_context_chunks` to `10`, save, and run again with the same question. Now there's too much context - it may be confusing or overwhelming. 

![Mod](./images/aia-1-51.png?raw=true "Mod")

<br>

Type `exit`. **Change it back to `3` (the sweet spot) and save it.**

![Mod](./images/aia-1-53.png?raw=true "Mod")


<br><br>

4. Now let's experiment with **temperature** - this controls how creative vs consistent the LLM is. Find the `generate()` method around line 236. Change `"temperature": 0.3` to `0.0` (very deterministic). Save the file.

![Temperature setting](./images/aia-1-58.png?raw=true "Temperature setting")

<br><br>

5. Run the system and ask the same question **twice**:

```
python rag_code.py
```

Ask twice: `What are the shipping costs?`

The answers should be nearly identical - temperature 0.0 gives consistent results. Type `exit`.

![Low temperature setting](./images/aia-1-59.png?raw=true "Low temperature setting")

<br><br>

6. Change temperature to `1.7` (very creative), save, and run again. Ask the same question twice. Notice the answers may vary quite a bit more - different wording, different order, and not as *professional*. This is less predictable. Type `exit` and change temperature back to `0.3`.

<br>

![Temperature setting](./images/aia-1-57.png?raw=true "Temperature setting")

<br>

![High temperature output](./images/aia-1-56.png?raw=true "High temperature output")

<br>

**Type `exit` and change temperature back to `0.3`.**

<br><br>

7. Now let's see **what the LLM actually sees**. Find the `query()` method around line 317. Right after the line `prompt = self.build_prompt(question, context_chunks)`, add this debug code:

```python
        # DEBUG: Show what the LLM actually sees
        print("\n" + "="*60)
        print("DEBUG: PROMPT SENT TO LLM")
        print("="*60)
        print(prompt)
        print("="*60 + "\n")
```

Save the file.

![Adding debug output](./images/aia-1-60.png?raw=true "Adding debug output")

<br><br>

8. Run the system and ask a question:

```
python rag_code.py
```

Ask: `How can I return a product?`

<br><br>

9. You'll now see the complete prompt including the system instructions, the 3 retrieved context chunks with sources, and your question. This is the "Augmentation" in RAG - augmenting your question with relevant context.

![Prompt visualization](./images/aia-1-61.png?raw=true "Prompt visualization")

<br><br>

10. Try another question to see how the context changes:

```
What are the shipping costs?
```

Notice how different chunks are retrieved, but the prompt structure stays the same. The context adapts to each question!

<br><br>

11. When done, type `exit`. You can remove or comment out the debug print statements (add `#` before each line you added in step 7) to clean up the output for production use.

<br><br>

**Key Takeaways:**
- **Chunk count (k)** affects answer quality - too few chunks miss context, too many add noise. 3-5 is often optimal.
- **Temperature** controls consistency vs creativity - low (0.1) for deterministic answers, high (0.9) for varied responses, medium (0.3-0.5) for balance.
- **RAG augmentation** adds retrieved context to your prompt before sending to the LLM - visualizing this helps you debug and improve results.
- The same question can retrieve different context chunks, showing how semantic search adapts to each query.

<p align="center">
<b>[END OF LAB]</b>
</p>
</br></br>


<p align="center">
<b>For educational use only by the attendees of our workshops.</b>
</p>

<p align="center">
<b>(c) 2025 Tech Skills Transformations and Brent C. Laster. All rights reserved.</b>
</p>
