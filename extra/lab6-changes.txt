#!/usr/bin/env python3
"""
Hybrid RAG: Combining Semantic Search + Knowledge Graph
================================================================================

This lab demonstrates TRUE hybrid RAG (Retrieval-Augmented Generation) by
combining two complementary retrieval methods:

1. SEMANTIC SEARCH (ChromaDB)
   - Uses vector embeddings to find documents with similar MEANING
   - Handles vocabulary mismatch (e.g., "money back" matches "refund")
   - Great for natural language queries and finding related content
   - Powered by ChromaDB vector database

2. GRAPH SEARCH (Neo4j)
   - Uses Cypher queries to traverse entity RELATIONSHIPS
   - Finds precise, structured facts via explicit connections
   - Great for "who handles X?" or "what applies to Y?" queries
   - Powered by Neo4j graph database

3. HYBRID (Both Combined)
   - Graph provides PRECISION (exact facts, timeframes, contacts)
   - Semantic provides CONTEXT (explanations, related information)
   - Together they give accurate AND comprehensive answers

KEY INSIGHT:
   Semantic search understands MEANING (natural language flexibility)
   Graph search understands STRUCTURE (entity relationships)
   Hybrid RAG combines both for production-grade accuracy

PREREQUISITES:
   - ChromaDB populated with OmniTech documents (run index_pdfs.py first)
   - Neo4j running with OmniTech graph (./neo4j-setup.sh 3)
   - Ollama running with llama3.2:3b model (ollama pull llama3.2:3b)

USAGE:
   python lab6_hybrid.py
   Then enter queries to compare all three methods side-by-side.
"""

# ══════════════════════════════════════════════════════════════════════════════
# IMPORTS
# ══════════════════════════════════════════════════════════════════════════════

import requests                 # For making HTTP requests to Ollama LLM API
from typing import List, Dict   # Type hints for better code readability

# ChromaDB imports for semantic/vector search
# PersistentClient connects to a disk-based ChromaDB database
from chromadb import PersistentClient
from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE

# py2neo is a Python library for working with Neo4j graph databases
# Graph class provides a connection to Neo4j and methods to run Cypher queries
from py2neo import Graph

# ══════════════════════════════════════════════════════════════════════════════
# CONFIGURATION
# These settings control where to find each database and which LLM to use
# ══════════════════════════════════════════════════════════════════════════════

# Path to the ChromaDB database directory (created by index_pdfs.py)
# This contains vector embeddings of all PDF document chunks
CHROMA_PATH = "./chroma_db"

# Name of the collection within ChromaDB that holds our document embeddings
# This was set when running index_pdfs.py
COLLECTION_NAME = "pdf_documents"

# Neo4j connection URI - Bolt protocol on default port 7687
# Neo4j must be running (./neo4j-setup.sh 3) for graph search to work
NEO4J_URI = "neo4j://localhost:7687"

# Neo4j authentication credentials
# These match what's set in neo4j-setup.sh
NEO4J_AUTH = ("neo4j", "neo4jtest")

# Ollama API endpoint for LLM generation
# Ollama must be running locally (ollama serve)
OLLAMA_URL = "http://localhost:11434/api/generate"

# Which Ollama model to use for generating answers
# llama3.2:3b is a good balance of speed and quality for demos
OLLAMA_MODEL = "llama3.2:3b"


# ══════════════════════════════════════════════════════════════════════════════
# HYBRID RAG CLASS
# This is the main class that implements all three search methods and LLM generation
# ══════════════════════════════════════════════════════════════════════════════

class HybridRAG:
    """
    Complete RAG system combining semantic search, knowledge graph, and LLM generation.

    This class provides three retrieval methods:
    - semantic_search(): Vector similarity search in ChromaDB
    - graph_search(): Cypher query traversal in Neo4j
    - hybrid_search(): Combines both methods

    And one generation method:
    - rag_query(): Full RAG pipeline (retrieve + augment + generate)
    """

    def __init__(self):
        """
        Initialize the HybridRAG system by connecting to both databases.

        On initialization, we:
        1. Connect to ChromaDB and load the document collection
        2. Connect to Neo4j and verify the graph is accessible

        If Neo4j is unavailable, the system continues with semantic-only search.
        """
        # These will hold our database connections
        self.collection = None  # ChromaDB collection for semantic search
        self.graph = None       # Neo4j graph connection for graph search

        # Establish connections to both databases
        self._connect_chromadb()
        self._connect_neo4j()

    # ──────────────────────────────────────────────────────────────────────────
    # DATABASE CONNECTION METHODS
    # ──────────────────────────────────────────────────────────────────────────

    def _connect_chromadb(self):
        """
        Connect to ChromaDB for semantic (vector) search.

        ChromaDB stores document chunks as vector embeddings. When we search,
        it finds chunks whose vectors are mathematically similar to the query
        vector - this is how it understands "meaning" rather than just keywords.

        The database must already exist (created by running index_pdfs.py).
        """
        print("Connecting to ChromaDB...")

        # PersistentClient connects to a database stored on disk
        # This is different from an in-memory client - data persists between runs
        client = PersistentClient(
            path=CHROMA_PATH,           # Directory where ChromaDB stores its data
            settings=Settings(),         # Default settings
            tenant=DEFAULT_TENANT,       # Multi-tenancy support (using default)
            database=DEFAULT_DATABASE    # Database name (using default)
        )

        # Get the collection that contains our PDF document embeddings
        # This collection was created by index_pdfs.py with the same name
        self.collection = client.get_collection(name=COLLECTION_NAME)

        # Report how many document chunks are available for search
        count = self.collection.count()
        print(f"  Loaded {count} document chunks")

    def _connect_neo4j(self):
        """
        Connect to Neo4j knowledge graph for structured search.

        Neo4j stores entities (nodes) and their relationships (edges).
        For example: Product -[APPLIES_TO]-> Policy -[HAS_TIMEFRAME]-> TimeFrame

        Cypher queries can traverse these relationships to find precise answers
        that would require parsing multiple documents in semantic search.

        If Neo4j isn't running, graph search will be disabled but the system
        continues to work with semantic search only.
        """
        print("Connecting to Neo4j...")
        try:
            # Create a connection to the Neo4j database
            # py2neo's Graph class handles the Bolt protocol connection
            self.graph = Graph(NEO4J_URI, auth=NEO4J_AUTH)

            # Verify the connection works by running a simple Cypher query
            # This also tells us how many nodes are in the graph
            result = self.graph.run("MATCH (n) RETURN count(n) as count").data()
            count = result[0]['count'] if result else 0
            print(f"  Loaded {count} graph nodes")

        except Exception as e:
            # If Neo4j isn't available, we degrade gracefully
            # The system will still work, just without graph search capabilities
            print(f"  Warning: Neo4j connection failed: {e}")
            print("  Graph search will be disabled. Run: cd neo4j && ./neo4j-setup.sh 3")
            self.graph = None

    # ══════════════════════════════════════════════════════════════════════════
    # SEMANTIC SEARCH (ChromaDB)
    # Uses vector embeddings to find documents with similar MEANING
    # ══════════════════════════════════════════════════════════════════════════

    def semantic_search(self, query: str, k: int = 3) -> List[Dict]:
        """
        Perform semantic search using ChromaDB vector embeddings.

        HOW IT WORKS:
        1. The query text is converted to a vector embedding (by ChromaDB)
        2. ChromaDB finds document chunks with similar vector embeddings
        3. Similarity is measured by cosine distance (lower = more similar)

        WHY IT'S USEFUL:
        - Understands meaning, not just keywords
        - "refund policy" matches "money back guarantee"
        - Handles natural language variations

        LIMITATIONS:
        - May return related but not directly relevant content
        - Precise facts may be buried in returned text
        - Quality depends on how well documents were chunked

        Args:
            query: The user's question or search text
            k: Number of results to return (default 3)

        Returns:
            List of dictionaries containing:
            - content: The actual text of the document chunk
            - metadata: Source file, page number, etc.
            - score: Relevance score (0-1, higher is better)
            - source: "semantic" to identify the retrieval method
        """
        # Query ChromaDB for similar document chunks
        # query_texts: The text to search for (converted to vector internally)
        # n_results: How many chunks to return
        # include: What data to return for each result
        results = self.collection.query(
            query_texts=[query],                              # Search query
            n_results=k,                                      # Number of results
            include=["documents", "metadatas", "distances"]   # What to return
        )

        # Transform ChromaDB's response format into our standard format
        # ChromaDB returns nested lists because it supports batch queries
        # We only have one query, so we access index [0] for everything
        return [
            {
                "content": results["documents"][0][i],      # The actual text
                "metadata": results["metadatas"][0][i],     # Source info
                "score": 1.0 / (1.0 + results["distances"][0][i]),  # Convert distance to score
                "source": "semantic"                        # Tag as semantic result
            }
            for i in range(len(results["documents"][0]))
        ]

    # ══════════════════════════════════════════════════════════════════════════
    # GRAPH SEARCH (Neo4j)
    # Uses Cypher queries to traverse entity RELATIONSHIPS
    # ══════════════════════════════════════════════════════════════════════════

    def graph_search(self, query: str, k: int = 5) -> List[Dict]:
        """
        Perform graph search using Neo4j Cypher queries.

        HOW IT WORKS:
        1. Analyze the query to determine what type of information is needed
        2. Construct a Cypher query that traverses relevant relationships
        3. Execute the query and format the results

        THE POWER OF GRAPH TRAVERSAL:
        Instead of searching text, we traverse explicit relationships:

            "What's the return window for Pro-Series?"

            Cypher traverses:
            Product(Pro_Series) <-[APPLIES_TO]- Policy(Pro_Series_Return)
                                -[HAS_TIMEFRAME]-> TimeFrame(14_Days)

            Direct answer: 14 days (no text parsing needed!)

        WHY IT'S USEFUL:
        - Precise, structured answers
        - Relationship queries ("who handles X?")
        - Consistent results for modeled entities

        LIMITATIONS:
        - Only knows explicitly modeled relationships
        - Requires upfront knowledge graph construction
        - Can't handle queries outside the graph schema

        Args:
            query: The user's question
            k: Maximum number of results to return

        Returns:
            List of dictionaries with content, metadata, score, source
        """
        # If Neo4j isn't connected, return empty results
        # The system will fall back to semantic search only
        if not self.graph:
            return []

        # Convert query to lowercase for keyword matching
        query_lower = query.lower()
        results = []

        # ──────────────────────────────────────────────────────────────────
        # QUERY ROUTING
        # Based on keywords in the query, we route to different Cypher queries
        # This is a simple approach - production systems might use NLU
        # ──────────────────────────────────────────────────────────────────

        if "pro-series" in query_lower or "pro series" in query_lower or "enterprise" in query_lower:
            # ══════════════════════════════════════════════════════════════
            # PRO-SERIES QUERY
            # Find policies that apply to Pro-Series products
            # ══════════════════════════════════════════════════════════════

            # If the query mentions "return", filter to only Return-type policies
            # Otherwise get all policies (Return, Warranty, etc.)
            policy_filter = "pol.type = 'Return'" if "return" in query_lower else "TRUE"

            # Cypher query explanation:
            # MATCH: Find Pro_Series product and policies that APPLY_TO it
            # WHERE: Filter to Return policies if query mentions returns
            # OPTIONAL MATCH: Also get related timeframes, conditions, fees, contacts
            # WHERE tf.description: Only get "return window" timeframe, not "refund processing"
            # RETURN DISTINCT: Avoid duplicate rows
            cypher = f"""
            MATCH (prod:Product {{name: 'Pro_Series'}})<-[:APPLIES_TO]-(pol:Policy)
            WHERE {policy_filter}
            OPTIONAL MATCH (pol)-[:HAS_TIMEFRAME]->(tf:TimeFrame)
            WHERE tf.description CONTAINS 'return' OR tf.description IS NULL
            OPTIONAL MATCH (pol)-[:REQUIRES_CONDITION]->(cond:Condition)
            OPTIONAL MATCH (pol)-[:HAS_FEE]->(fee:Fee)
            OPTIONAL MATCH (contact:Contact)-[:HANDLES]->(pol)
            RETURN DISTINCT prod, pol, tf, cond, fee, contact
            """

            # Execute the Cypher query
            data = self.graph.run(cypher).data()

            # Deduplicate results by policy name
            # (Multiple rows might exist for the same policy with different contacts)
            seen_policies = set()
            for row in data:
                if row.get('pol') and row['pol']['name'] not in seen_policies:
                    seen_policies.add(row['pol']['name'])
                    results.append(self._format_policy_result(row))

        elif "defective" in query_lower or "broken" in query_lower or "doa" in query_lower:
            # ══════════════════════════════════════════════════════════════
            # DEFECTIVE/DOA QUERY
            # Find the DOA (Defective on Arrival) policy and contact info
            # ══════════════════════════════════════════════════════════════

            # This is a RELATIONSHIP query - we traverse:
            # DOA_Policy -[HAS_TIMEFRAME]-> TimeFrame (reporting window)
            # DOA_Policy -[APPLIES_TO]-> Products (what it covers)
            # Contact -[HANDLES]-> DOA_Policy (who to contact)
            cypher = """
            MATCH (pol:Policy {name: 'DOA_Policy'})
            OPTIONAL MATCH (pol)-[:HAS_TIMEFRAME]->(tf:TimeFrame)
            OPTIONAL MATCH (pol)-[:APPLIES_TO]->(prod:Product)
            OPTIONAL MATCH (contact:Contact)-[:HANDLES]->(pol)
            RETURN pol, tf, prod, contact
            """
            data = self.graph.run(cypher).data()
            for row in data:
                results.append(self._format_doa_result(row))

        elif "contact" in query_lower or "email" in query_lower or "phone" in query_lower or "who" in query_lower:
            # ══════════════════════════════════════════════════════════════
            # CONTACT QUERY
            # Find all contacts and what policies they handle
            # ══════════════════════════════════════════════════════════════

            # This query finds all Contact nodes and collects what they handle
            # collect() aggregates multiple matches into a list
            cypher = """
            MATCH (c:Contact)
            OPTIONAL MATCH (c)-[:HANDLES]->(target)
            RETURN c, collect(target) as handles
            """
            data = self.graph.run(cypher).data()
            for row in data:
                results.append(self._format_contact_result(row))

        elif "return" in query_lower or "refund" in query_lower:
            # ══════════════════════════════════════════════════════════════
            # GENERAL RETURN POLICY QUERY
            # Find all return-type policies with their timeframes and products
            # ══════════════════════════════════════════════════════════════
            cypher = """
            MATCH (pol:Policy)
            WHERE pol.type = 'Return'
            OPTIONAL MATCH (pol)-[:HAS_TIMEFRAME]->(tf:TimeFrame)
            OPTIONAL MATCH (pol)-[:APPLIES_TO]->(prod:Product)
            RETURN pol, tf, collect(DISTINCT prod) as products
            """
            data = self.graph.run(cypher).data()
            for row in data:
                results.append(self._format_return_result(row))

        else:
            # ══════════════════════════════════════════════════════════════
            # GENERIC FALLBACK QUERY
            # If no specific pattern matches, return some policies with timeframes
            # ══════════════════════════════════════════════════════════════
            cypher = """
            MATCH (pol:Policy)-[:HAS_TIMEFRAME]->(tf:TimeFrame)
            RETURN pol, tf
            LIMIT 5
            """
            data = self.graph.run(cypher).data()
            for row in data:
                pol = row['pol']
                tf = row['tf']
                results.append({
                    "content": f"Policy: {pol['name']}\nDescription: {pol['description']}\nTimeframe: {tf['value']}",
                    "metadata": {"node": pol['name'], "type": "Policy"},
                    "score": 0.5,
                    "source": "graph"
                })

        # Return at most k results
        return results[:k]

    # ──────────────────────────────────────────────────────────────────────────
    # GRAPH RESULT FORMATTERS
    # These methods convert Neo4j query results into our standard format
    # ──────────────────────────────────────────────────────────────────────────

    def _format_policy_result(self, row) -> Dict:
        """
        Format a policy query result into readable content.

        Takes a row from a Cypher query that includes policy, timeframe,
        fee, condition, and contact information, and formats it as
        human-readable text for the LLM context.
        """
        pol = row['pol']
        content = f"Policy: {pol['name']}\n"
        content += f"Description: {pol['description']}\n"

        # Add optional fields if they exist in the query result
        if row.get('tf'):
            content += f"Return Window: {row['tf']['value']}\n"
        if row.get('fee'):
            content += f"Restocking Fee: {row['fee']['value']} - {row['fee']['description']}\n"
        if row.get('cond'):
            content += f"Required Condition: {row['cond']['name']} - {row['cond']['description']}\n"
        if row.get('contact'):
            c = row['contact']
            content += f"Contact: {c['value']}"
            if c.get('hours'):
                content += f" ({c['hours']})"
            content += "\n"

        return {
            "content": content.strip(),
            "metadata": {"node": pol['name'], "type": "Policy"},
            "score": 1.0,       # Graph results get high confidence score
            "source": "graph"   # Tag as graph result
        }

    def _format_doa_result(self, row) -> Dict:
        """Format a DOA (Defective on Arrival) policy result."""
        pol = row['pol']
        content = f"Policy: {pol['name']}\n"
        content += f"Description: {pol['description']}\n"
        if row.get('tf'):
            content += f"Report Within: {row['tf']['value']}\n"
        if row.get('contact'):
            c = row['contact']
            content += f"Contact: {c['value']} ({c['department']})"
        return {
            "content": content,
            "metadata": {"node": pol['name'], "type": "Policy"},
            "score": 1.0,
            "source": "graph"
        }

    def _format_contact_result(self, row) -> Dict:
        """Format a contact query result with what they handle."""
        c = row['c']
        content = f"Contact: {c['name']}\n"
        content += f"Value: {c['value']}\n"
        content += f"Type: {c['type']}\n"
        content += f"Department: {c['department']}\n"
        if c.get('hours'):
            content += f"Hours: {c['hours']}\n"
        if row.get('handles'):
            # List all policies/entities this contact handles
            handles = [h['name'] for h in row['handles'] if h]
            if handles:
                content += f"Handles: {', '.join(handles)}"
        return {
            "content": content,
            "metadata": {"node": c['name'], "type": "Contact"},
            "score": 1.0,
            "source": "graph"
        }

    def _format_return_result(self, row) -> Dict:
        """Format a return policy result with applicable products."""
        pol = row['pol']
        content = f"Policy: {pol['name']}\n"
        content += f"Description: {pol['description']}\n"
        if row.get('tf'):
            content += f"Window: {row['tf']['value']}\n"
        if row.get('products'):
            prods = [p['name'] for p in row['products'] if p]
            if prods:
                content += f"Applies To: {', '.join(prods)}"
        return {
            "content": content,
            "metadata": {"node": pol['name'], "type": "Policy"},
            "score": 1.0,
            "source": "graph"
        }

    # ══════════════════════════════════════════════════════════════════════════
    # HYBRID SEARCH
    # Combines graph precision with semantic context
    # ══════════════════════════════════════════════════════════════════════════

    def hybrid_search(self, query: str, k: int = 3) -> List[Dict]:
        """
        Perform hybrid search combining graph and semantic results.

        HOW IT WORKS:
        1. Run graph search to get precise, structured facts
        2. Run semantic search to get contextual document chunks
        3. Combine results with graph first (precision), then semantic (context)

        WHY COMBINE BOTH:
        - Graph provides exact facts: "14 days", "returns@company.com"
        - Semantic provides context: "conditions for returns", "exceptions"
        - Together: accurate facts WITH full explanations

        Args:
            query: The user's question
            k: Number of results from each method

        Returns:
            Combined list with graph results first, then semantic results
        """
        # Get precise facts from the knowledge graph
        graph_results = self.graph_search(query, k=k)

        # Get contextual information from document embeddings
        semantic_results = self.semantic_search(query, k=k)

        # Combine results: graph first for precision, then semantic for context
        # This ordering helps the LLM prioritize structured facts
        combined = graph_results + semantic_results

        # Return up to 2*k results (k from each method)
        return combined[:k*2]

    # ══════════════════════════════════════════════════════════════════════════
    # LLM GENERATION (The "G" in RAG)
    # Uses retrieved context to generate accurate, grounded answers
    # ══════════════════════════════════════════════════════════════════════════

    def generate_answer(self, question: str, context_chunks: List[Dict]) -> str:
        """
        Generate an answer using Ollama LLM with retrieved context.

        This is the GENERATION step of RAG:
        1. Retrieved chunks are formatted into a context prompt
        2. The LLM generates an answer grounded in that context
        3. The answer is based on actual data, reducing hallucination

        PROMPT STRUCTURE:
        - STRUCTURED DATA: Facts from the knowledge graph
        - DOCUMENT CONTEXT: Relevant chunks from semantic search
        - QUESTION: The user's original question
        - INSTRUCTIONS: How to use the context to answer

        Args:
            question: The user's original question
            context_chunks: Retrieved chunks from search methods

        Returns:
            The LLM-generated answer string
        """
        # Separate graph and semantic results for different formatting
        graph_context = [c for c in context_chunks if c.get("source") == "graph"]
        semantic_context = [c for c in context_chunks if c.get("source") == "semantic"]

        # Build the context section of the prompt
        context_parts = []

        # Add graph results as "STRUCTURED DATA" - these are precise facts
        if graph_context:
            context_parts.append(
                "STRUCTURED DATA (from knowledge graph):\n" +
                "\n---\n".join([c["content"] for c in graph_context])
            )

        # Add semantic results as "DOCUMENT CONTEXT" - these provide explanations
        if semantic_context:
            context_parts.append(
                "DOCUMENT CONTEXT:\n" +
                "\n---\n".join([
                    f"[{c['metadata'].get('source', 'doc').split('/')[-1]}]\n{c['content'][:500]}"
                    for c in semantic_context
                ])
            )

        context = "\n\n".join(context_parts)

        # Construct the full prompt for the LLM
        # This prompt template instructs the LLM how to use the context
        prompt = f"""Based on the following information, answer the user's question.

{context}

QUESTION: {question}

Provide a helpful, accurate answer. Use the structured data for precise facts (timeframes, contacts)
and the document context for explanations. If information isn't available, say so."""

        # Send request to Ollama API
        try:
            response = requests.post(
                OLLAMA_URL,
                json={
                    "model": OLLAMA_MODEL,      # Which model to use
                    "prompt": prompt,            # The full prompt with context
                    "stream": False,             # Get complete response at once
                    "options": {"temperature": 0.3}  # Lower = more focused/deterministic
                },
                timeout=60  # Wait up to 60 seconds for response
            )

            if response.status_code == 200:
                return response.json().get("response", "").strip()
            return f"Error: {response.status_code}"

        except Exception as e:
            return f"LLM Error: {e}"

    # ══════════════════════════════════════════════════════════════════════════
    # COMPLETE RAG PIPELINE
    # Retrieve → Augment → Generate
    # ══════════════════════════════════════════════════════════════════════════

    def rag_query(self, question: str, method: str = "hybrid") -> Dict:
        """
        Execute the complete RAG pipeline: Retrieve → Augment → Generate.

        This is the main entry point for querying the system:
        1. RETRIEVE: Get relevant chunks using specified method
        2. AUGMENT: Format chunks into context for the LLM prompt
        3. GENERATE: Call LLM to produce a grounded answer

        Args:
            question: The user's question
            method: Which retrieval method to use:
                    - "semantic": ChromaDB vector search only
                    - "graph": Neo4j Cypher queries only
                    - "hybrid": Both combined (default)

        Returns:
            Dictionary containing:
            - answer: The LLM-generated response
            - chunks: The retrieved context chunks
            - method: Which method was used
        """
        # RETRIEVE: Get relevant context using the specified method
        if method == "semantic":
            chunks = self.semantic_search(question)
        elif method == "graph":
            chunks = self.graph_search(question)
        else:
            chunks = self.hybrid_search(question)

        # AUGMENT + GENERATE: Build prompt with context and call LLM
        answer = self.generate_answer(question, chunks)

        return {
            "answer": answer,
            "chunks": chunks,
            "method": method
        }


# ══════════════════════════════════════════════════════════════════════════════
# TERMINAL COLOR CODES
# ANSI escape codes for colored terminal output
# ══════════════════════════════════════════════════════════════════════════════

RED = "\033[91m"     # Semantic results displayed in red
BLUE = "\033[94m"    # Graph results displayed in blue
GREEN = "\033[92m"   # Hybrid results displayed in green
RESET = "\033[0m"    # Reset to default terminal color
BOLD = "\033[1m"     # Bold text


# ══════════════════════════════════════════════════════════════════════════════
# INTERACTIVE DEMO
# Allows users to enter queries and compare all three methods side-by-side
# ══════════════════════════════════════════════════════════════════════════════

def run_demo():
    """
    Run an interactive demo comparing all three RAG methods.

    This function:
    1. Initializes the HybridRAG system
    2. Prompts the user for queries
    3. Runs each query through semantic, graph, and hybrid methods
    4. Displays color-coded results for easy comparison
    5. Continues until user types 'exit'

    Color coding:
    - RED: Semantic search results (ChromaDB)
    - BLUE: Graph search results (Neo4j)
    - GREEN: Hybrid results (both combined)
    """
    # Display header
    print(f"{BOLD}Hybrid RAG: Semantic + Graph + Hybrid{RESET}")
    print("=" * 50)

    # Initialize the RAG system (connects to both databases)
    rag = HybridRAG()

    # Warn if Neo4j isn't available
    if not rag.graph:
        print(f"\n{RED}Neo4j not connected. Graph search disabled.{RESET}")
        print("Run: cd /workspaces/rag/neo4j && ./neo4j-setup.sh 3\n")

    # Display instructions
    print(f"\nEnter queries to compare all 3 methods. Type 'exit' to quit.\n")
    print(f"  {RED}RED{RESET} = Semantic (ChromaDB)")
    print(f"  {BLUE}BLUE{RESET} = Graph (Neo4j)")
    print(f"  {GREEN}GREEN{RESET} = Hybrid (Both)\n")

    # Main interaction loop
    while True:
        # Get query from user
        query = input(f"{BOLD}Query:{RESET} ").strip()

        # Exit condition
        if query.lower() == "exit":
            print("Goodbye!")
            break

        # Skip empty queries
        if not query:
            continue

        print()

        # ──────────────────────────────────────────────────────────────────
        # Run query through all three methods and display results
        # ──────────────────────────────────────────────────────────────────

        # SEMANTIC: Vector similarity search in ChromaDB
        print(f"{RED}{BOLD}SEMANTIC:{RESET}")
        sem_result = rag.rag_query(query, method="semantic")
        print(f"{RED}{sem_result['answer']}{RESET}")
        print()

        # GRAPH: Cypher query traversal in Neo4j
        print(f"{BLUE}{BOLD}GRAPH:{RESET}")
        graph_result = rag.rag_query(query, method="graph")
        print(f"{BLUE}{graph_result['answer']}{RESET}")
        print()

        # HYBRID: Both methods combined
        print(f"{GREEN}{BOLD}HYBRID:{RESET}")
        hybrid_result = rag.rag_query(query, method="hybrid")
        print(f"{GREEN}{hybrid_result['answer']}{RESET}")
        print()

        # Separator between queries
        print("-" * 50)
        print()


# ══════════════════════════════════════════════════════════════════════════════
# MAIN ENTRY POINT
# ══════════════════════════════════════════════════════════════════════════════

if __name__ == "__main__":
    # Run the interactive demo when script is executed directly
    run_demo()


